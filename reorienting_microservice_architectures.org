#+TITLE: Reorienting Microservice Architectures
#+HTML_LINK_HOME: ./index.html
#+HTML_LINK_UP: ./index.html

Microservice architectures (MSAs) are a widely pursued
buzzword. Although most of the guidance seems to be "try anything else
first", many organizations eagerly dive in the hopes that MSAs
offer some kind of solution in terms of scale, or cost, or
organizational friction. Any of these ideas are valid...with a few
pages worth of criteria.

** Popping Some Bubbles

In terms of both scale and cost some of the major considerations are
in terms of higher initial overhead and the Pareto Principle. MSAs out
of the gate have higher costs (in all forms) and likely lower
performance due to more expensive communication across nodes which has
adverse implications on being able to cost effectively scale. The
Pareto Principle is likely the more significant consideration as the
assertion that the ability to scale pieces of your infrastructure
independently is undermined by the notion that there are likely to be
relatively few hot spots. This counterargument is furthered by the
notion that latency sensitive apps also have bursty traffic patterns
and therefore a cluster of those hot spots is likely to have
sufficient spare cycles within a typical time window to handle the
~20% of lower volume traffic. This mirrors some of the benefits
espoused by co-locating containers which are often wrapped in
misconceptions around how such work will be isolated and resources
will be shared - while attention should be paid to concerns such as
process scheduling such behavior can be configured at the level of the
OS (which is where it is handled in containers) or the relevant VM. In
terms of organizational structure, MSAs can be a useful _tool_ but are
not a solution. A naive adoption of microservices will not deliver the
desired autonomy and agility but simply obscure the inherited friction
and the overall system is likely to amount to a "distributed
monolith".  Conway's Law can be a useful guiding framework for these
types of decisions but too often it seems to be given lip service
without deeper inspection with the idea that independent teams will
somehow bestow independence on the systems that they own, rather than
the two needing to be defined together; tactics like the Reverse
Conway Maneuver should be driven by data and design rather than
optimism otherwise any existing drag will simply be amplified as it
passes through both systems and people. This manifests in both
development time (coordinating on what to build) and runtime (tracking
the successful delivery of value to users).  There are clear benefits
_iff_ there is independence which microservices can help reinforce but
not enable.

Another quick and hopefully obvious concern to track is around
simplicity. The cognitive overhead of designing and operating
microservices tends to be notably higher. There can be benefits in
constraining the amount of logic and concepts present in a given
context, but the benefits of such chunking does not naturally
translate to distributing the systems themselves.

** A Saner Perspective

One of the main contributing factors to the prospect of Microservices
is the simple feasibility of the approach. Launching an arbitrary
number of instances of free open source software does not raise
licensing costs, and many of the concerns around orchestrating and
running such an architecture have been worked out at this
point. Unfortunately the initial adoption of Microservices seems to
have jumped over one of the most valuable potentials in this world in
that it can deliver the _decoupling of software from how and where it
is deployed._ Through this lens, the application of microservices as
an organizational driver rather than more of a technical detail poses
higher coupling and commensurate lower agility.

Rather than orienting around the notion of "microservices" the focus
should be on the already well established, more abstract, division of
"modules". Perhaps the crucial distinction is by preserving the
freedom to modify what a software does and how it is deployed, the
technical headaches of microservices can be addressed with more
dedication. While domain knowledge is typically the heaviest lift in
designing software and leveraging approaches such as defined Bounded
Contexts can provide massive benefits, attempting to apply such
practices across all levels contradicts some of the more fundamental
precepts concerning isolating business logic from infrastructural
concerns.

By recognizing that many of the modern tools and practices can be
leveraged not as a means to realize particular deployment strategies
but rather to more freely mix and move across such options, the
trade-offs can be assessed orthogonally along the axes of domain and
technical boundaries. On the most basic level the assertion that
logical seperation should carry through to physical separation
(i.e. hardware) seems tenuous and, within the context of modern
tooling, flawed and in fairly direct conflict with the typical model
involving generalized compute which likely goes all the way down to
the random-access machine model underlying most application
development.

Reestablishing such separation allows the deployment model and the
stickier concerns that arise with distributed systems to be assessed
from a technical perspective. Within a larger system and often within
given bounded domain contexts there are likely to be boundaries that
can be established. Concerns such as data locality, transactionality,
failure domains, resource usage, and non-functional
requirements/architectural characteristics are all likely to be very
useful in assessing the deployment model and as may be evident there
could easily be a many-to-many mapping between such concerns and the
domain contexts which the machine is serving, and accordingly the
owners of the logic.

A major prerequisite of the purported agility afforded by this
separation is precise requirements (which is arguably always a
prerequisite for safe agility along with corresponding
verification). Both functional and non-functional requirements should
be refined to clearly express what is _necessary_, otherwise systems
are likely to end up locked in what may have been an incidental
deployment decision due to the concerns outlined above. This is often
one of the major contributors to the aforementioned trap of a
distributed monolith in that if the requirements don't clearly
articulate the particulars of given use cases then the system may end
up contorted to match assumptions or an inappropriate topology.

** Operability

An aspect which seems diverting to is operation of a running
system. This was briefly mentioned earlier and is also implicit in
many of the other sentiments. The underlying premise here is being
able to change deployment models and therefore operational concerns
become another parameter in making such decisions. There is a
simplistic perception that microservices and a relationship to team
boundaries can aid in operations but there are several factors that
may inhibit that. As with other considerations the overall overhead
increases as things are distributed and there should therefore be a
path to the breakeven point which also takes into consideration people
and skill sets. In addition to more general goals such as
observability this can also manifest in the form of expertise around
how particular focused technologies are being used or potentially how
more generalized technologies are being stretched. Additionally, as
mentioned earlier, clear visiblity into a single system does not
provide inherent value if it is a piece of a larger puzzle and
therefore from a business perspective may lead to passing the buck or
pointing fingers.

This concern seems worth individual attention since I've noticed it
raised specifically in conversations, but it fits the same pattern as
other points herein in that the resulting state tends to reflect and
often amplify the underlying forces rather than benefiting from any
magic sauce.  If you have clear visibility into your overall system
and are able to smoothly work with it then you're doing the right
thing, if you don't then it should be handled as a first-class concern
rather than expecting it to by a byproduct of a buzzword.

** In Practice

All of the above speaks to the desire to decouple software and the
associated development efforts from how it is deployed, and a natural
way to gauge the effectiveness of such a pursuit is deployment
flexibility. There are at least four distinct deployment models that
come to mind:

- a library in a monolith or other deployment
- a container within a scheduled pod
- a standalone service
- function as a service

If code can be faily easily moved between some of these options then
the desired agiliity has been realized. The decision to adopt one over
the other can therefore be more easily evolved in light of some of the
underlying tradeoffs, requirements, and data.

The principles underlying much of this are not new by any stretch; if
there's a separation of concerns which makes use of modularity and
inversion of control then the core logic should be decoupled from
infrastructural concerns and ultimately the difference between any of
the above amount to such infrastructural decisions. Such principles
unfortunately often seem neglected, but proper adherence (and
potential enforcement) should provide a good baseline for such
independence.

From what I gather this idea seems to be at the heart of the Clojure
"Polylith" project which one of my colleages references periodically
but I honestly still haven't had the time to investigate with any
depth. As implied I tend to view much of this as a natural consequence
of good design rather than requiring any additional machinery (which
may be the case for many things).

In terms of a wider organization this will manifest more concretely in
a "library-first" design principle. The produced library artifact
could then be pulled in for use in any of the environments listed
above, and as it evolves many of the deployment containers could also
be largely standardized such that they do not require additional
coding. The library itself should follow what should also be standard
expectations around libraries - minimal dependencies that fit within
an IS-A relationship and therefore intended for use with inversion of
both control and dependencies.

#  LocalWords:  MSAs VM Polylith

