#+TITLE: Reorienting Microservices

Microservice architectures (MSAs) are a widely pursued
buzzword. Although most of the guidance seems to be "try anything else
first", many organizations eagerly dive in the hopes that MSAs
offer some kind of solution in terms of scale, or cost, or
organizational friction. Any of these ideas are valid...with a few
pages worth of criteria.

** Popping Some Bubbles

In terms of both scale and cost some of the major considerations are
in terms of higher initial overhead and the Pareto Principle.  MSAs
out of the gate have higher costs (in all forms) and likely lower
performance due to more expensive communication across nodes which has
adverse impacts on being able to scale cost effectively. The Pareto
Principle is likely the more significant consideration as the
assertion that the ability to scale pieces of your infrastructure
independently is undermined by the notion that there are likely to be
relatively few hot spots. This counterargument is furthered by the
notion that latency sensitive apps also have bursty traffic patterns
and therefore a cluster of those hot spots is likely to have
sufficient spare cycles at any given time to handle the ~20% of lower
volume traffic. This mirrors some of the benefits espoused by
co-locating containers which are often wrapped in misconceptions around
how such work will be isolated and resources will be shared - while
attention should be paid to concerns such as process scheduling such
behavior can be configured at the level of the OS (which is where it
is handled in containers) or the relevant VM. In terms of
organizational structure, MSAs can be a useful _tool_ but are not a
solution. A naive adoption of microservices will not deliver the
desired autonomy and agility but simply obscure the inherited friction
and the overall system is likely to amount to a "distributed
monolith".  Conway's Law can be a useful guiding framework for these
types of decisions but too often it seems to be given lip service
without deeper inspection with the idea that independent teams will
somehow bestow independence on the systems that they own, rather than
the two needing to be defined together; tactics like the Reverse
Conway Maneuver should be driven by data and design rather than
optimism otherwise any existing drag will simply be amplified as it
passes through both systems and people. This manifests in both
development time (coordinating on what to build) and runtime (tracking
the successful delivery of value to users).  There are clear benefits
_iff_ there is independence which microservices can help
reinforce but not enable.

Another quick and hopefully obvious concern to track is around
simplicity. The cognitive overhead of designing and operating
microservices tends to be notably higher. There can be benefits in
constraining the amount of logic and concepts present in a given
context, but the benefits of such chunking does not naturally
translate to distributing the systems themselves.

** A Saner Perspective

One of the main contributing factors to the prospect of Microservices
is the simple feasibility of the approach. Launching an arbitrary
number of instances of free open source software does not raise
licensing costs, and many of the concerns around orchestrating and
running such an architecture have been worked out at this
point. Unfortunately the initial adoption of Microservices seems to
have jumped over one of the most valuable potentials in this world in
that it can deliver the _decoupling of software from how and where it
is deployed._ Through this lens, the application of microservices as
an organizational driver rather than more of a technical detail poses
higher coupling and commensurate lower agility.

Rather than orienting around the notion of "microservices" the focus
should be on the already well established, more abstract, division of
"modules". Perhaps the crucial distinction is by preserving the
freedom to modify what a software does and how it is deployed, the
technical headaches of microservices can be addressed with more
dedication. While domain knowledge is typically the heaviest lift in
designing software and leveraging approaches such as defined Bounded
Contexts can provide massive benefits, attempting to apply such
practices across all levels contradicts some of the more fundamental
precepts concerning isolating business logic from infrastructural
concerns.

By recognizing that many of the modern tools and practices can be
leveraged not as a means to realize particular deployment strategies
but rather to more freely mix and move across such options, the
trade-offs can be assessed orthogonally along the axes of domain and
technical boundaries. On the most basic level the assertion that
logical seperation should carry through to physical separation
(i.e. hardware) seems tenuous and, within the context of modern
tooling, flawed and in fairly direct conflict with the typical model
involving generalized compute which likely goes all the way down to
the random-access machine model underlying most application
development.

Reestablishing such separation allows the deployment model and the
stickier concerns that arise with distributed systems to be assessed
from a technical perspective. Within a larger system and often within
given bounded domain contexts there are likely to be boundaries that
can be established. Concerns such as data locality, transactionality,
failure domains, resource usage, and non-functional
requirements/architectural characteristics are all likely to be very
useful in assessing the deployment model and as may be evident there
could easily be a many-to-many mapping between such concerns and the
domain contexts which the machine is serving, and accordingly the
owners of the logic.

A major prerequisite of the purported agility afforded by this
separation is precise requirements (which is arguably always a
prerequisite for safe agility along with corresponding
verification). Both functional and non-functional requirements should
be refined to clearly express what is _necessary_, otherwise systems
are likely to end up locked in what may have been an incidental
deployment decision due to the concerns outlined above. This is often
one of the major contributors to the aforementioned trap of a
distributed monolith in that if the requirements don't clearly
articulate the particulars of given use cases then the system may end
up contorted to match assumptions or an inappropriate topology.

** Operability

An aspect which seems diverting to is operation of a running
system. This was briefly mentioned earlier and is also implicit in
many of the other sentiments. The underlying premise here is being
able to change deployment models and therefore operational concerns
become another parameter in making such decisions. There is a
simplistic perception that microservices and a relationship to team
boundaries can aid in operations but there are several factors that
may inhibit that. As with other considerations the overall overhead
increases

#  LocalWords:  MSAs VM
