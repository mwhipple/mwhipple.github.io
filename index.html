<html lang="en-US">
<head>
  <meta charset="UTF-8" />
  <link rel="icon" type="image/x-icon" href="./favicon.ico" />
  <link rel="canonical" href="https://www.mattwhipple.com" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="description" content="Share knowledge and thoughts about software and...whatever."/>
  <meta name="keywords" content="software" />
  <meta name="resource-type" content="document" />
  <meta name="distribution" content="global" />
  <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1" />
  <title>Home | Matt Whipple</title>
</head>

<body lang="en">

<div class="overview">

  <h1>Home</h1>

  <p>Welcome to the latest iteration of my Web site.</p>

  <p>I'll be copying information from previous incarnations.</p>

</div>

<div class="section">

  <h2>Information Collection</h2>

  <p>
    To gamify information consumption in the interest of this site a bit for myself
    I'm going to look to use only information which is navigable from source that I've read
    (rather than from issuing a search).
  </p>

  <p>
    One of the initial roots for the search will be Martin Kleppmann's excellent
    <a href="#src-ddia">Designing Data-Intensive Applications</a> which is not only a great resource
    by itself but also a treasure trove of <a href="#src-ddia-references">useful references</a>.
    In addition to this I'll be using books that I have or discover at libraries and information read
    from assorted memberships to O'Reilly, ACM, IEEE and the New York Times.
  </p>

</div>

<div class="section">

  <h2>Software Selection</h2>

  <p>
    The software used will also be driven by what can be discovered through navigation,
    with a useful initial directory being provided by the <a href="#src-gnumanuals">GNU Manuals</a>.
    (and a more comprehensive selection in the linked Free Software Directory which will be
    explored to fill possible gaps).
  </p>

</div>

<div class="section">

  <h2>Markup</h2>

  <p>
    The initial markup will start by stealing basic practices from sources used, so far:
    <ul>
      <li><a href="#src-gnucoreutils_manual">GNU Coreutils Manual</a></li>
      <li><a href="$src-github_blog">GitHub Blog</a></li>
    </ul>
  </p>

</div>

<div class="section">

  <h2>GitHub</h2>

  <p>
    Initially the source for this site will be hosted on GitHub. I personally have more interest in some other alternatives
    which I'll look to explore for any projects that I publish, but GitHub is what I'm currently using for work and also
    is the most readily navigable service from my Information Collection approach, starting with some GitHub blog posts.
  </p>

</div>

<div class="section">

  <h2>GitHub Pages</h2>

  <p>
    This site will first be hosted on <a href="#github_blog_pages">GitHub Pages</a>. This is a solid option overall and
    one I've used in the past. I will likely be looking to move away from it for reasons that will be elucidated if and
    when that happens (nothing overly important).
  </p>

</div>

<div class="section">

  <h2>Google App Engine</h2>

  <p>
    <a href="#src-gae">Google App Engine</a> is a viable hosting option that came up fairly early in my information navigation
    and seems to potentially fit the right set of criteria. Google Cloud is also a technology I'll be using for work and one which I
    haven't spent too much time (like much of the industry I'm far more comfortable with AWS).
  </p>

  <p>
    I used and evangelized GAE a fair amount quite a while ago (prior to and during the period when it went General Availability),
    and would be likely to use it again if I were looking to develop an appropriate app. While I'm currently looking to use the
    flexible model for containerized deployment I've generally been of the mindset that adapting designs to fit within the constraints
    of systems like GAE can provide significant upfront benefits. Typically the aversion to such technologies that I've seen seems to be
    a fixed mindset where people are turned off that they can't use their preferred hammers, which isn't to say that that isn't often
    a compelling reason, particularly for an organization. Similarly there may be some cases for which a particular type of technology
    is most appropriate but an awkward fit. Additionally I haven't run the numbers for GAE in particular but in most
    similar systems there's a breakeven point after which it is likely to be far more cost effective to use something closer to the
    metal even after accounting for the operational overhead. But...as mentioned the <em>upfront</em> benefits can be substantial and
    other than cultural resistance (which isn't a concern for my personal projects) many of the other concerns are initially aspirational.
  </p>

  <p>
    For this particular project I'll be looking to make use of the <a href="#src-gae-flexible">flexible environment</a> as I'm very
    comfortable with dealing with containers and I'll be starting to use a non-supported runtime and likely evolving into making use
    of a relatively standard container which is also not directly supported, so in both cases relying on a
    <a href="#src-gae-custom">custom runtime</a>. An enticing longer term plan may be to switch to
    <a href="#src-gae-cloud-run">Cloud Run</a> as it is likely to be lower cost at low volume, but in the immediate term
    it seems like a distraction and I may also want to look at more static-centric alternatives in the future.
  </p>

</div>

<div class="section">

  <h2>nginx</h2>

  <p>
    I'll be looking to deploy this site as a static site hosted through nginx as outlined in the Google App Engine flexible
    runtime <a href="#src-gae-quickstart">quickstart</a>. I've spent a fair amount of time fiddling with
    nginx and while I'm not particularly fond of their business model (particularly the somewhat crippled open source offering)
    the software itself has proven to be very robust and efficient with wide adoption...and I have assorted nginx stickers and
    T-shirts (one of which is black-on-black and probably my favorite tech shirt) so I should probably validate the swag by
    using the software. My past experience also gives me confidence that I can bend it to my wishes in some way or another and
    maybe learn some new tricks along the way (I already have a module sitting around to address the DNS resolution issue which
    may otherwise dissuade me).
  </p>

  <h2>Artanis (Old)</h2>

  <p>
    This site was going to will looking to be running on top of <a href="#src-gnuartanis">GNU Artanis</a>. This is largely a consequence
    of the approach to Information Collection, although the project may potentially align with some longer term goals.
    I'm more likely to ultimately land on a solution that is more closely aligned with a static Web site/JAM stack,
    but I think I also want the site to have some basic support for concepts such as content negotiation which many static
    solutions don't lend themselves naturally to and therefore I'll probably ultimately move to something like nginx
    (which is also recommended in front of Artanis but I haven't navigated my way over there yet). I enjoy Scheme but
    am likely to gravitate more towards Python, I'll elaborate upon some of my larger thoughts around languages in an
    upcoming update.
  </p>

  <p>
    Relatively up-to-date information on Artanis seems to be provided on its <a href="#src-artanis_github">GitLab page</a>
    (including containerized execution which I'll be looking to build on top of).
  </p>

  <p>
    Artanis itself is built on top of <a href="#src-gnuguile">GNU Guile</a> which was installed but not detected on my system
    (it is potentially outdated), so I may also dig into that project though I'll likely start with seeing about containerized
    Artanis.
  </p>

  <h3>Containerized Execution</h3>

  <p>
    I'll be looking to run Artanis in a container, and accordingly the initially desired development environment is also
    using containers to most closely align with the deployment (installing anything on my host is effectively more of a
    workflow optimization). There are <a href="#src-nalaginrut-artanis-docker">some instructions</a> provided for running
    Artanis in a container, but those instructions need to be tuned a bit. Most significantly the tag for the image is
    outdated and should instead be more of the form <code>registry.gitlab.com/hardenedlinux/artanis</code> to reflect the
    project being transferred. Updated guidance is provided with the
    <a href="#src-nalaginrut-artanis_0.5_docker">instructions for Artanis 0.5.</a>
  </p><p>
    Beyond that there are a few adjustments for the sake of usability and reproducibility:
    the image should have an appropriate entrypoint so that it can be launched directly and an ostensibly immutable tag
    should be used (where the latter concern transcends the scope of the blog post). The entrypoint decision is somewhat
    a matter of preference or perspective where the instructions seem to be more oriented towards using the container to
    provide an interactive contained environment which takes care of dependencies whereas my preference would be more of a directly
    invokable utility container which is more targeted towards providing a service to the host (where the entrypoint can
    always be overridden if needed). I'll therefore likely create an image for local development which adjusts the entrypoint
    whereas I'd typically look to just use the upstream image for that purpose (along with expected tweaks to the command.
    There's also a decent chance the image could be slimmed down a bit if it's not expected to be interactive, but it doesn't
    seem <em>overly</em> large given the use case and so that feels like premature optimization for the time being
    (it seems unnecessarily large given the functionality for a deployment that is expected to scale but I presently have no reason to
    think this site fits that criterion).
  </p><p>
    The standard upstream image also seems to fall into a fairly common trap of using root rather than a less-privileged user.
    I think this is covered as part of Dockerfile best practices that I'll need to chase down.
  </p>

  <p>
    Another fairly general note is that the guidance on the post suggests using <a href="#src-docker-get"><code>docker</code></a>
    but I'd probably gravitate towards an alternative like <code>nerdctl</code>. This is relatively insignificant as it can be abstracted
    by the Makefile but seems worth mentioning given the strong sentiment in the Artanis documentation and the GNU
    community as a whole towards free software against which the licensing for Docker Desktop <em>may</em> run afoul.
  </p>

  <h4>Details</h4>

  <p>
    My starting point is to use the provided image and access a locally running site...unforutnately some of my quick tweaks didn't
    quite land so I'll need to quickly retrieve specifics from the <a href="#src-docker-reference">Docker reference documentation</a>
    since I typically can remember what options there are but not quite how they look. While I (as mentioned) am looking to move away
    from Docker it has good documentation and remains the de facto standard at the moment so starting with what is documented for Docker
    is certainly a safe point while pursuing what is currently Docker-compatible invocations.
  </p>

  <p>
    To get the ball rolling I just fiddled with the command to get it to work, but I'll adjust them based on the
    <a href="#src-docker-run">docker run documentation</a> and replace the short options with the longer forms.
  </p>

</div>

<div class="section">

  <h2>Programming Languages are Harmful</h2>

  <h3>Backstory</h3>

  <p>
    Over my career I've programmed in a range of different languages and I'm currently nursing an idea that languages offer tenuous
    return on investment to the software industry. Don't get me wrong, I certainly think many languages have contributed many great
    ideas and I'm also vehemently opposed to panaceas and golden hammers and therefore recognize that some problems are best explored
    using appropriate means of expression.
  </p><p>
    The back-breaking straw of this idea was likely spending a chunk of one of my days spelunking through Python's SSL implementation and 
    corresponding C code to work through a bug
    whose behavior I could readily reproduce and resolve using the <code>openssl</code> command. While I used this as a chance to
    brush up on some TLS details the main lingering impression was the large tracts of code that existed not to do anything new, but
    simply to reexpress an established solution (the particular issue was resolved by swapping out LibreSSL for OpenSSL).
  </p><p>
    Some other recent background context was likely being engaged in several conversations around language adoption, and somewhat
    relatively recently within an engineering culture that had a <q>one language to rule them all</q> mindset even when the output
    of that mindset was often bizarre paths of using that language to invoke shell commands to launch other languages or containers
    (which I looked to simplify as things broke...which they often did).
  </p>

  <h3>Software Tools and Monoliths</h3>

  <p>
    A well-known exchange in the annals of computing history is Doug McIlroy's criticism of a program Don Knuth wrote to demonstrate
    literate programming for Jon Bentey's <q>Programming Pearls</q> CACM column (with the underlying premise that if software is treated
    as literature it should be subjected to criticism). While there is a fair amount to glean from all of the material, one of the main
    takeaways is that McIlroy was able to counter Knuth's multi-page application with a short and fairly basic Unix pipeline.
    The prospect of software tools composed with uniform pipes offered an approach which could be more quickly produced, more readily
    evolved, and was more able to tap portable background knowledge.
  </p><p>
    As alluded to the discussion is a bit more nuanced and as one of my former coworkers posited that argument is on much shakier ground
    when considering a system such as TeX (and I personally am a proponent of literate programming), but it is worth recognizing that all
    too often programming languages seem to drift towards such monolithic recreations of functionality which could otherwise be more readily
    composed. From a more modern perspective many of the software tools ideas carry forward to pursuits such as microserves...or less
    ambitiously any use of well designed utility containers.
  </p>

  <h3>The What and the How</h3>

  <p>
    At its core, programming is largely about algorithms and algorithms are the imperative complement to declarative knowledge.
    A clear and often used example (I think it appears in sources such as The Art of Computer Programming and The Go Programming Language)
    is <em>how</em> Euclid's Algorithm can be coded to determine <em>what</em> is the greatest common divisor (GCD) of two numbers.
    The GCD is an important declared concept and the automation of how to derive the appropriate value is enormously valuable.
  </p><p>
    While automating such processes is the core value of computers, one of the most valuable tools in engineering is that of
    abstraction. Abstraction is essential for making systems comprehensible and while it is crucial in all fields of engineering,
    it is integral to software engineering - not only is the base level of software engineering built atop many layers of electrical
    engineering abstractions, but the creation of abstractions is one of the core tools of software engineers.
    Abstraction effectively buries the previously mentioned core value of computing; drawing on the earlier example while the value of
    <em>computers</em> is the automation of the <em>how</em> of Euclid's Algorithm, the value delivered by the software is that
    of producing the <em>what</em> of the GCD. Therefore the produced machine is most simply represented as a <em>what</em> producing
    blackbox so the GCD and any other relevant pieces of data can be used without getting swamped in details.
  </p><p>
    Unfortunately languages are often too focused on aspects of <em>how</em> - all too often there are established and abstracted solutions
    for particular problems but all too often significant additional effort is required to reimplement or adapt such solutions so that
    they conform with <em>how</em> a particular language seeks to express ideas within the solution space. This perspective in isolation is
    a bit simplistic but will be elaborated upon further.
  </p>

  <h3>Systems vs. Application Programming</h3>

  <p>
    A key distinction when discussing programming is that between systems programming and application programming. The official definitions
    should be incorporated here, but for in the short term I'll consider systems programming as that which is focused on enabling use of what
    is immediately available. At the lowest level this involves things like hardware and instruction sets but more typically this is
    likely to make use of the facilities provided by the operating system or higher level of abstraction such as virtual machines. The output
    of systems programming is likely to be a generalized machine.
  </p><p>
    Applications programming is developing software to solve specific business cases...which is the vast majority of modern software being
    created. Drawing on Fred Brooks's <q>No Silver Bullet</q>, the vast majority of concerns introduced by software in applications programming
    amount to accidental complexity: friction which is introduced by the solution space where only a small amount is likely to be an essential
    consideration for the problem at hand. Unfortunately software engineers seem to gravitate towards challenges that may be technically
    interesting but have unclear business value. A basic smell test for such considerations is whether their removal would confer a competitive
    advantage - if you're investing in establishing deep technical expertise but a competitor made use of an alternative which avoided that
    overhead would they be able to undercut you? If that resonates then it is a strong indication of an accretion of accidental complexity
    which is likely to escalate both delivery and maintenance costs. No-code, low-code, and ML-assisted development may help reduce smooth out
    some of these edges but a far more fundamental adjustment is simply to recognize the difference between systems and application programming
    and the natural corollary that unless your core deliverable is software itself rather than the output of such software, any and all effort
    expended in producing such output with the desired characteristics is cost and should be managed appropriately. There's a larger thread
    lurking within this sentiment around accountability and how perverse some routine engineering efforts would seem if analogs within other
    departments existed but are accepted in the magical world of software, but that's likely a larger topic.
  </p><p>
    Much of this ties directly back to the concepts in <q>The What and the How</q>: application programming is best expressed entirely in
    terms of <em>what</em>s. Preferring more declarative interfaces can make machines far more economical to develop and easier to reason about,
    verify, and optimize. Most application programming, however, tends to remain largely imperative without a clear delineation from systems
    programming. This break is certainly not a clearly achievable goal and is likely to echo the largely unrealized dream of fourth generation
    languages, <em>but</em> a key distinction in this thread is that the focus is emphatically not about pursuing a language but rather reuse
    and composition of abstractions. This also touches back upon <q>Software Tools and Monoliths</q> and echoes some of Martin Kleppmann's
    sentiments at the end of <a href="#src-ddai">Desiging Data-Intensive Applications</a> which imagines being able to compose data flows
    as easily as Unix pipelines.
  </p>

  <h3>Underlying Concerns</h3>

  <p>
    The previous ideas stay pretty conceptual but all of these ideas need to meet the metal at some point and some of the things that
    have been glossed over need warrant some attention.
  </p><p>
    Perhaps one of the first places to start is that much of the described interoperability is available in the form of 
    foreign function interfaces...but foreign? If two languages are running on the same system what is foreign? This cuts back to
    the Python SSL code that pushed me over this ledge - why was so much code necessary to call a library that I could easily interact
    with from the command line?
  </p><p>
    One of the clearest initial issues is the representation of data within any given language. Most instruction sets operate on
    bits and while they may care about the number of bits involved they are far less likely to care about what the bits mean; semantics
    that extend beyond basics such as a sign bit are likely to be largely handled by convention. While low level languages like C provide
    some behavior on top of this such behavior is largely in the compiler so the data worked with is still largely bits rather than
    having any particularly defined metadata. As most major operating systems are written in a C compatible language this leaves
    higher level languages to track what additional data they need either alongside the data itself or in separate structures, establishing
    local conventions which then need a translation layer for "foreign" data.
  </p><p>
    A related but arguably less fundamental concern is around some of the additional behavior provided by particular languages.
    Concerns such as memory management or safety may require that any data which is not provided through standard channels must be
    somehow taken into the fold. This is significant but ultimately this should correspond to some combination of the aforementioned
    metadata and perhaps a function invocation. There are certainly stickier considerations if data is registered for such functionality
    while remaining accessible to external code, but those are detail devils that are ultimately the result of some of the initial
    practices which are being questioned (if such functionality were orthogonal to some notion of "language" then the foreign/external
    boundary disappears.
  </p><p>
    When considering established approaches such as unix pipelines a hidden concern is that of inter-process communication (IPC).
    Consistently sharing data by value is wonderfully simple...but can be fairly costly. The same concern manifests in microservices
    and the goals of GNU Hurd. On a given system this is another incarantion of the previous concern: any perceived <em>requirement</em>
    that the value must be handled in a particular way is the result of it crossing some potentially arbitrary boundary.
  </p>

  <h2>Where This Leads</h2>

  <p>
    I'll be exploring some of the options with some of the ideas above, much of this circles around the notion of relying upon an
    established low level interface to avoid some of the translation boundaries, where the clearest initial such interface would
    be the SysV ABI. Particular features of given languages would be implemented independently of the language itself, and the
    means of expression which often seem to dominate current languages would be a layer on top of all of these (like a lightweight
    compiler frontend). This will probably result primarily in adoption of tools which match these goals (I feel like I may be
    getting pulled toward C++) with some gaps filled by some homemade glue (particularly for some of the additional tastes like
    literate programming).
  </p>

</div>

<div class="section">

  <h2>Microservices</h2>

  <p>
    Microservice architectures (MSAs) are a widely pursued buzzword. Although most of the guidance seems to be <q>try anything else first</q>,
    many organizations eagerly dive in in the hopes that MSAs offer some kind of solution in terms of scale, or cost, or organizational friction.
    Any of these ideas are valid...with a few pages worth of criteria.
  <p>

  <h3>Popping Some Bubbles</h3>

  <p>
    In terms of both scale and cost some of the major considerations are in terms of higher initial overhead and the Pareto Principle.
    MSAs out of the gate have higher costs (in all forms) and likely lower performance due to more expensive communication across nodes
    which has adverse impacts on being able to scale cost effectively. The Pareto Principle is likely the more significant consideration
    as the assertion that the ability to scale pieces of your infrastructure independently is undermined by the notion that there are likely
    to be relatively few hot spots. This counterargument is furthered by the notion that latency sensitive apps also have bursty traffic
    patterns and therefore a cluster of those hot spots is likely to have sufficient spare cycles at any given time to handle the ~20% of
    lower volume traffic. This mirrors some of the benefits espoused by colocating containers which are often wrapped in misconceptions around
    how such work will be isolated and resources will be shared - while attention should be paid to concerns such as process scheduling
    such behavior can be configured at the level of the OS (which is where it is handled in containers) or the relevant VM.
  </p>

  <p>
    In terms of organizational structure, MSAs can be a useful <em>tool</em> but are not a solution. A naive adoption of microservices will
    not deliver the desired autonomy and agility but simply obscure the inherited friction and the overall system is likely to amount to
    a "distributed monolith".  Conway's Law can be a useful guiding framework for these types of decisions but too often it seems to be
    given lip service without deeper inspection with the idea that independent teams will somewhow bestow independence on the systems that
    they own, rather than the two needing to be defined together; tactics like the Reverse Conway Maneuver should be driven by data
    and design rather than optimism otherwise any existing drag will simply be amplified as it passes through both systems and people.
    This manifests in both development time (coordinating on what to build) and runtime (tracking the successul delivery of value to users).
    There are clear benefits <em>iff</em> there is independence which microservices can help reinforce but not enable.
  </p>

  <p>
    Another quick and hopefully obvious concern to track is around simplicity. The cognitive overhead of designing and operating microservices
    tends to be notably higher. There can be benefits in constraining the amount of logic and concepts present in a given context, but
    the benefits of such chunking does not naturally translate to distributing the systems themselves.
  </p>

  <h3>A Saner Perpsective</h3>

  <p>
    One of the main contributing factors to the prospect of Microservices is the simple feasibility of the approach. Launching an arbitrary
    number of instances of free open source software does not raise licensing costs, and many of the concerns around orchestrating and
    running such an architecture have been worked out at this point. Unfortunately the initial adoption of Microservices seems to have jumped
    over one of the most valuable potentials in this world in that it can deliver the <em>decoupling of software from how and where it is deployed.</em>
    Through this lens, the application of microservices as an organizational driver rather than more of a technical detail poses higher coupling
    and commensurate lower agility.
  </p>

  <p>
    Rather than orienting around the notion of "microservices" the focus should be on the already well established, more abstract, divison
    of "modules". Perhaps the crucial distinction is by preserving the freedom to modify what a software does and how it is deployed, the
    technical headaches of microservices can be addressed with more dedication. While domain knowledge is typically the heaviest lift in
    designing software and leveraging approaches such as defined Bounded Contexts can provide massive benefits, attempting to apply
    such practices across all levels contradicts some of the more fundamental precepts concerning isolating business logic from
    infrastructural concerns.
  </p>

  <p>
    By recognizing that many of the modern tools and practices can be leveraged not as a means to realize particular deployment strategies
    but rather to more freely mix and move across such options, the tradeoffs can be assessed orthogonally along the axes of domain and
    technical boundaries. On the most basic level the assertion that logical seperation should carry through to physical separation (i.e. hardware)
    seems tenuous and, within the context of modern tooling, flawed and in fairly direct conflict with the typical model involving generalized
    compute which likely goes all the way down to the random-access machine model underlying most application development.
  </p>

  <p>
    Restablishing such separation allows the deployment model and the stickier concerns that arise with distributed systems to be assessed 
    from a technical perspective. Within a larger system and often within given bounded domain contexts there are likely to be boundaries
    that can be established. Concerns such as data locality, transactionality, failure domains, resource usage,
    and non-functional requirements/architectural characteristics are all likely to be very useful in assessing the deployment model and as
    may be evident there could easily be a
    many-to-many mapping between such concerns and the domain contexts which the machine is serving, and accordingly the owners of the logic.
  </p>

  <p>
    A major prerequisite of the purported agility afforded by this separation is precise requirements (which is arguably always a prerequisite
    for safe agility along with corresponding verification). Both functional and non-functional requirements should be refined to clearly
    express what is <em>necessary</em>, otherwise systems are likely to end up locked in what may have been an incidental deployment decision
    due to the concerns outlined above. This is often one of the major contributors to the aforementioned trap of a distributed monolith in that
    if the requirements don't clearly articulate the particulars of given use cases then the system may end up contorted to match assumptions
    or an inappropriate topology.
  </p>

  <h3>Operability</h3>

  <p>
    An aspect which seems diverting to is operation of a running system. This was briefly mentioned earlier and is also implicit in many of
    the other sentiments. The underlying premise here is being able to change deployment models and therefore operational concerns become
    another parameter in making such decisions. There is a simplistic perception that microservices and a relationship to team boundaries
    can aid in operations but there are several factors that may inhibit that. As with other considerations the overall overhead increases
    as things are distributed and there should therefore be a path to the breakeven point which also takes into consideration people and
    skillsets. In addition to more general goals such as observability this can also manifest in the form of expertise around how particular
    focused technologies are being used or potentially how more generalized technologies are being stretched. Additionally, as mentioned earlier,
    clear visiblity into a single system does not provide inherent value if it is a piece of a larger puzzle and therefore from a business
    perspective may lead to passing the buck or pointing fingers. 
  </p>

  <p>
    This concern seems worth individual attention since I've noticed it raised specifically in conversations, but it fits the same pattern as
    other points herein in that the resulting state tends to reflect and often amplify the underlying forces rather than benefiting from any magic sauce.
    If you have clear visibility into your overall system and are able to smoothly work with it then you're doing the right thing,
    if you don't then it should be handled as a first-class concern rather than expecting it to by a byproduct of a buzzword.
  </p>

  <h3>In Practice</h3>

  <p>
    All of the above speaks to the desire to decouple software and the associated development efforts from how it is deployed, and a natural
    way to gauge the effectiveness of such a pursuit is deployment flexibility. There are at least four distinct deployment models that come
    to mind:
    <ul>
      <li>a library in a monolith or other deployment</li>
      <li>a container within a scheduled pod</li>
      <li>a standalone service</li>
      <li>function as a service</li>
    </ul>
    If code can be faily easily moved between some of these options then the desired agiliity has been realized. The decision to adopt
    one over the other can therefore be more easily evolved in light of some of the underlying tradeoffs, requirements, and data.
  </p>

  <p>
    The principles underlying much of this are not new by any stretch; if there's a separation of concerns which makes use of modulariy
    and inversion of control then the core logic should be decoupled from infrastructural concerns and ultimately the difference between
    any of the above amount to such infrastructural decisions. Such principles unfortunately often seem neglected, but proper adherence
    (and potential enforcement) should provide a good baseline for such independence.
  </p>

  <p>
    From what I gather this idea seems to be at the heart of the Clojure <em>Polylith</em> project which one of my colleages references
    periodically but I honestly still haven't had the time to investigate with any depth. As implied I tend to view much of this as a
    natural consequence of good design rather than requiring any additional machinery (which may be the case for many things).
  </p>

  <p>
    In terms of a wider organization this will manifest more concretely in a "library-first" design principle. The produced library
    artifact could then be pulled in for use in any of the environments listed above, and as it evolves many of the deployment
    containers could also be largely standardized such that they do not require additional coding. The library itself should follow
    what should also be standard expectations around libraries - minimal dependencies that fit within an IS-A relationship and
    therefore intended for use with inversion of both control and dependencies.
  </p>

</div>

<div class="section">

  <h2>References</h2>

  <ol class="sources">
    <li><a id="src-ddia">Designing Data-Intensive Applications</a></li>
    <li><a id="src-ddia-references" data-in="src-ddia" href="https://github.com/ept/ddia-references">ddia-references</a></li>
    <li><a id="src-gnucoreutils_manual" data-in="src-ddia" href="https://www.gnu.org/software/coreutils/manual/html_node/index.html">GNU Coreutils Manual</a></li>
    <li><a id="src-gnumanuals" data-in="src-gnucoreutils_manual" href="https://www.gnu.org/manual">GNU Manuals Online</a></li>
    <li><a id="src-github_blog" data-in="src-ddia" href="https://github.blog">GitHub Blog</a></li>
    <li><a id="src-gnuartanis" data-in="src-gnumanuals" href="https://www.gnu.org/software/artanis">GNU Artanis</a></li>
    <li><a id="src-gnuguile" data-in="src-gnumanuals" href="https://www.gnu.org/software/guile">GNU Guile</a></li>
    <li><a id="src-github_blog_pages" data-in="src-github_blog" href="https://github.blog/2008-12-18-github-pages">GitHub Pages | The GitHub Blog</a></li>
    <li><a id="src-artanis_gitlab" data-in="src-gnuartanis" href="https://gitlab.com/hardenedlinux/artanis">HardenedLinux / artanis</a></li>
    <li><a id="src-github_blog_rebase_9" data-in="src-github_blog" href="https://github.blog/2008-12-22-github-rebase-9">GitHub Rebase #9 | The GitHub Blog</a></li>
    <li><a id="src-gae" data-in="src-github_blog_rebase_9" href="https://cloud.google.com/appengine/">App Engine Application Platform | Google Cloud</a></li>
    <li><a id="src-gae-flexible" data-in="src-gae" href="https://cloud.google.com/appengine/docs/flexible">
      Google App Engine flexible environment docs | Google Cloud
    </a></li>
    <li><a id="src-gae-custom" data-in="src-gae-flexible" href="https://cloud.google.com/appengine/docs/flexible/custom-runtimes/about-custom-runtimes">
      About Custom runtimes | Google Cloud
    </a></li>
    <li><a id="src-gae-cloud-run" data-in="src-gae-flexible" href="https://cloud.google.com/appengine/docs/flexible/cloud-run-for-gae-customers">
      Cloud Run for App Engine customers | Google App Engine flexible environment docs | Google Cloud
    </a></li>
    <li><a id="src-gae-custom-quickstart" data-in="src-gae-flexible" href="https://cloud.google.com/appengine/docs/flexible/custom-runtimes/create-app">
      Quickstart: Create a custom runtime app in the App Engine flexible environment  |  Google App Engine flexible environment docs  |  Google Cloud
    </a></li>
    <li><a id="src-gcloud" href="https://cloud.google.com/sdk/docs" data-in="src-gae-custom-quickstart">
      Google Cloud CLI documentation | Google Cloud CLI Documentation
    </a></li>
    <li><a id="src-gcloud-quickstart" href="https://cloud.google.com/sdk/docs/install-sdk" data-in="src-gcloud">
      Quickstart: Install the Google Cloud CLI  |  Google Cloud CLI Documentation
    </a></li>

    <li><a id="src-nalaginrut" data-in="src-artanis_gitlab" href="https://nalaginrut.com">Samson's Machete</a></li>
    <li><a id="src-nalaginrut-artanis_docker" data-in="src-artanis_gitlab" href="https://nalaginrut.com/archives/2019/09/18/install%20gnu%20artanis%20with%20docker">
      Samson's Machete (Install GNU Artanis with Docker)
    </a></li>
    <li><a id="src-docker-get" data-in="src-nalaginrut-artanis_docker" href="https://docs.docker.com/get-docker/">Get Docker | Docker Documentation</a></li>
    <li><a id="src-docker-reference" data-in="src-docker-get" href="https://docs.docker.com/reference">Reference documentation | Docker Documentation</a></li>
    <li><a id="src-docker-cli" data-in="src-docker-reference" href="https://docs.docker.com/engine/reference/commandline/cli">Use the Docker command line | Docker Documentation</a></li>
    <li><a id="src-docker-run" data-in="src-docker-run" href="https://docs.docker.com/engine/reference/commandline/run">docker run | Docker Documentation</a></li>
    <li><a id="src-github_blog_git_site" data-in="src-github_blog" href="https://github.blog/2008-07-26-new-git-site/">
      New Git Site | The GitHub Blog
    </a></li>
    <li><a id="src-nalaginrut-artanis_0.5_docker" href="https://nalaginrut.com/archives/2021/02/16/gnu%20artanis-0.5%20is%20ready%20for%20docker">
      Samsons's Machete (GNU Artanis-0.5 is ready for docker)
    </a></li>
    <li><a id="src-nalaginrut-hurd" href="https://nalaginrut.com/archives/2019/12/11/hurd%2c%20sel4%2x%20thoughts">
      Samsons's Machete (Hurd, seL4, thoughts)
    </a></li>
  </ol>
</div>

</body>

</html>
